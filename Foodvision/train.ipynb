{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_files=\"./Custom_data.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "train_dataset = splits['train']\n",
    "test_dataset = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "model_checkpoint = \"Neruoy/swin-finetuned-food101-e3\"\n",
    "batch_size = 32\n",
    "\n",
    "image_processor  = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
    "image_processor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import (\n",
    "    Normalize,\n",
    "    Resize,\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "if \"height\" in image_processor.size:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "    print('Height')\n",
    "elif \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = image_processor.size.get(\"longest_edge\")\n",
    "    print('Shortest')\n",
    "\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "test_transfroms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_test(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [test_transfroms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_transform(preprocess_train)\n",
    "test_dataset.set_transform(preprocess_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at Neruoy/swin-finetuned-food101-e3 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([101, 1024]) in the checkpoint and torch.Size([10, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([101]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes = True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                                          Input Shape          Output Shape         Param #              Trainable\n",
       "================================================================================================================================================================\n",
       "SwinForImageClassification (SwinForImageClassification)                          [32, 3, 224, 224]    [32, 10]             --                   True\n",
       "├─SwinModel (swin)                                                               [32, 3, 224, 224]    [32, 1024]           --                   True\n",
       "│    └─SwinEmbeddings (embeddings)                                               [32, 3, 224, 224]    [32, 3136, 128]      --                   True\n",
       "│    │    └─SwinPatchEmbeddings (patch_embeddings)                               [32, 3, 224, 224]    [32, 3136, 128]      6,272                True\n",
       "│    │    └─LayerNorm (norm)                                                     [32, 3136, 128]      [32, 3136, 128]      256                  True\n",
       "│    │    └─Dropout (dropout)                                                    [32, 3136, 128]      [32, 3136, 128]      --                   --\n",
       "│    └─SwinEncoder (encoder)                                                     [32, 3136, 128]      [32, 49, 1024]       --                   True\n",
       "│    │    └─ModuleList (layers)                                                  --                   --                   86,734,648           True\n",
       "│    └─LayerNorm (layernorm)                                                     [32, 49, 1024]       [32, 49, 1024]       2,048                True\n",
       "│    └─AdaptiveAvgPool1d (pooler)                                                [32, 1024, 49]       [32, 1024, 1]        --                   --\n",
       "├─Linear (classifier)                                                            [32, 1024]           [32, 10]             10,250               True\n",
       "================================================================================================================================================================\n",
       "Total params: 86,753,474\n",
       "Trainable params: 86,753,474\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 5.74\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 9248.44\n",
       "Params size (MB): 346.76\n",
       "Estimated Total Size (MB): 9614.47\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=model, \n",
    "        input_size=(32, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                                          Input Shape          Output Shape         Param #              Trainable\n",
       "================================================================================================================================================================\n",
       "SwinForImageClassification (SwinForImageClassification)                          [32, 3, 224, 224]    [32, 10]             --                   Partial\n",
       "├─SwinModel (swin)                                                               [32, 3, 224, 224]    [32, 1024]           --                   False\n",
       "│    └─SwinEmbeddings (embeddings)                                               [32, 3, 224, 224]    [32, 3136, 128]      --                   False\n",
       "│    │    └─SwinPatchEmbeddings (patch_embeddings)                               [32, 3, 224, 224]    [32, 3136, 128]      (6,272)              False\n",
       "│    │    └─LayerNorm (norm)                                                     [32, 3136, 128]      [32, 3136, 128]      (256)                False\n",
       "│    │    └─Dropout (dropout)                                                    [32, 3136, 128]      [32, 3136, 128]      --                   --\n",
       "│    └─SwinEncoder (encoder)                                                     [32, 3136, 128]      [32, 49, 1024]       --                   False\n",
       "│    │    └─ModuleList (layers)                                                  --                   --                   (86,734,648)         False\n",
       "│    └─LayerNorm (layernorm)                                                     [32, 49, 1024]       [32, 49, 1024]       (2,048)              False\n",
       "│    └─AdaptiveAvgPool1d (pooler)                                                [32, 1024, 49]       [32, 1024, 1]        --                   --\n",
       "├─Linear (classifier)                                                            [32, 1024]           [32, 10]             10,250               True\n",
       "================================================================================================================================================================\n",
       "Total params: 86,753,474\n",
       "Trainable params: 10,250\n",
       "Non-trainable params: 86,743,224\n",
       "Total mult-adds (Units.GIGABYTES): 5.74\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 9248.44\n",
       "Params size (MB): 346.76\n",
       "Estimated Total Size (MB): 9614.47\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=model, \n",
    "        input_size=(32, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    model_checkpoint,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('mps:0')\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6482d051511e46398174dfd2265b5efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8353, 'learning_rate': 0.0004761904761904762, 'epoch': 0.14}\n",
      "{'loss': 0.2833, 'learning_rate': 0.0009523809523809524, 'epoch': 0.28}\n",
      "{'loss': 0.041, 'learning_rate': 0.0009523809523809524, 'epoch': 0.43}\n",
      "{'loss': 0.0636, 'learning_rate': 0.0008994708994708994, 'epoch': 0.57}\n",
      "{'loss': 0.0457, 'learning_rate': 0.0008465608465608465, 'epoch': 0.71}\n",
      "{'loss': 0.0525, 'learning_rate': 0.0007936507936507937, 'epoch': 0.85}\n",
      "{'loss': 0.043, 'learning_rate': 0.0007407407407407407, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc4ad95e73d48bbbc32127ec20099d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03788638487458229, 'eval_accuracy': 0.989, 'eval_runtime': 35.9026, 'eval_samples_per_second': 27.853, 'eval_steps_per_second': 0.891, 'epoch': 0.99}\n",
      "{'loss': 0.0251, 'learning_rate': 0.0006878306878306878, 'epoch': 1.13}\n",
      "{'loss': 0.031, 'learning_rate': 0.0006349206349206349, 'epoch': 1.28}\n",
      "{'loss': 0.0339, 'learning_rate': 0.000582010582010582, 'epoch': 1.42}\n",
      "{'loss': 0.0409, 'learning_rate': 0.0005291005291005291, 'epoch': 1.56}\n",
      "{'loss': 0.0225, 'learning_rate': 0.0004761904761904762, 'epoch': 1.7}\n",
      "{'loss': 0.0494, 'learning_rate': 0.00042328042328042324, 'epoch': 1.84}\n",
      "{'loss': 0.0316, 'learning_rate': 0.00037037037037037035, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548aa36723d74d8b8786064a380658b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04139889404177666, 'eval_accuracy': 0.99, 'eval_runtime': 36.577, 'eval_samples_per_second': 27.34, 'eval_steps_per_second': 0.875, 'epoch': 2.0}\n",
      "{'loss': 0.0173, 'learning_rate': 0.00031746031746031746, 'epoch': 2.13}\n",
      "{'loss': 0.0273, 'learning_rate': 0.00026455026455026457, 'epoch': 2.27}\n",
      "{'loss': 0.0256, 'learning_rate': 0.00021164021164021162, 'epoch': 2.41}\n",
      "{'loss': 0.0172, 'learning_rate': 0.00015873015873015873, 'epoch': 2.55}\n",
      "{'loss': 0.0281, 'learning_rate': 0.00010582010582010581, 'epoch': 2.7}\n",
      "{'loss': 0.0273, 'learning_rate': 5.2910052910052905e-05, 'epoch': 2.84}\n",
      "{'loss': 0.0172, 'learning_rate': 0.0, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40781c21f4554fbca4e506530d27eace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.035387810319662094, 'eval_accuracy': 0.991, 'eval_runtime': 35.9349, 'eval_samples_per_second': 27.828, 'eval_steps_per_second': 0.891, 'epoch': 2.98}\n",
      "{'train_runtime': 1101.7038, 'train_samples_per_second': 24.507, 'train_steps_per_second': 0.191, 'train_loss': 0.1313641648207392, 'epoch': 2.98}\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       2.98\n",
      "  train_loss               =     0.1314\n",
      "  train_runtime            = 0:18:21.70\n",
      "  train_samples_per_second =     24.507\n",
      "  train_steps_per_second   =      0.191\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./Mymodel/')\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e021fd3a3b4a6587905b9ffaf447c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       2.98\n",
      "  eval_accuracy           =      0.991\n",
      "  eval_loss               =     0.0354\n",
      "  eval_runtime            = 0:00:36.99\n",
      "  eval_samples_per_second =     27.033\n",
      "  eval_steps_per_second   =      0.865\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
